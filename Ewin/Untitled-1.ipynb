{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install findspark\n",
    "#/usr/local/opt/apache-spark/libexec/sbin\n",
    "#./start-thriftserver.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.server2.thrift.port\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/19 12:31:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"spark\").config('spark.driver.memory', '32g').config(\"hive.server2.thrift.port\", 10000).config(\"spark.sql.hive.thriftServer.singleSession\", True).enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Speed: double (nullable = true)\n",
      " |-- TravelTime: integer (nullable = true)\n",
      " |-- Status: integer (nullable = true)\n",
      " |-- timedate: string (nullable = true)\n",
      " |-- LinkId: integer (nullable = true)\n",
      " |-- LinkPoints: string (nullable = true)\n",
      " |-- EncodedLinkPoints: string (nullable = true)\n",
      " |-- EncodedPolyLineLvls: string (nullable = true)\n",
      " |-- Owner: string (nullable = true)\n",
      " |-- TranscomId: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Link_Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType,TimestampType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType()),\n",
    "    StructField(\"Speed\", DoubleType()),\n",
    "    StructField(\"TravelTime\", IntegerType()),\n",
    "    StructField(\"Status\", IntegerType()),\n",
    "    StructField(\"timedate\", StringType()),\n",
    "    StructField(\"LinkId\", IntegerType()),\n",
    "    StructField(\"LinkPoints\", StringType()),\n",
    "    StructField(\"EncodedLinkPoints\", StringType()),\n",
    "    StructField(\"EncodedPolyLineLvls\", StringType()),\n",
    "    StructField(\"Owner\", StringType()),\n",
    "    StructField(\"TranscomId\", IntegerType()),\n",
    "    StructField(\"Borough\", StringType()),\n",
    "    StructField(\"Link_Name\", StringType())\n",
    "])\n",
    "\n",
    "schema1 = \"ID bigint, Speed float, TravelTime int, Status int, TimeDate string, LinkId int, LinkPoints string, EncodedLinkPoints string, EncodedPolyLineLvls string, Owner string, TranscomId int, Borough string,Link_Name string\"\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(\"file:/Users/minoseah629/Downloads/DOT_Traffic_Speeds_NBE.csv\")\n",
    "df2.printSchema()\n",
    "# .option(\"inferSchema\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:======================================================>(189 + 3) / 192]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Speed: double (nullable = true)\n",
      " |-- TravelTime: integer (nullable = true)\n",
      " |-- Status: integer (nullable = true)\n",
      " |-- timedate: timestamp (nullable = true)\n",
      " |-- LinkId: integer (nullable = true)\n",
      " |-- LinkPoints: string (nullable = true)\n",
      " |-- EncodedLinkPoints: string (nullable = true)\n",
      " |-- EncodedPolyLineLvls: string (nullable = true)\n",
      " |-- Owner: string (nullable = true)\n",
      " |-- TranscomId: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Link_Name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumn(\"timedate\", to_timestamp(\"timedate\", 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df2.na.drop()\n",
    "df2.createOrReplaceTempView(\"dataset\")\n",
    "\n",
    "df2.count()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2017'\n",
    "start = year+'-01-01'\n",
    "end = year+'-12-31'\n",
    "query = \"select ID,Speed,TravelTime,Status,timedate,LinkId,LinkPoints,EncodedLinkPoints,EncodedPolyLineLvls,Owner,TranscomId,Borough,Link_Name from dataset\"\n",
    "where = \"where timedate >= '\"+start+\"' and timedate <= '\"+end+\"' order by timedate asc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2017 = spark.sql(query + \" \"+where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "y2017.write.format('csv').mode('overwrite').save('file:/Users/minoseah629/Desktop/ait614/1year'+year+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = \"timedate, speed, traveltime\"\n",
    "select = \"select \"+columns #+\",count(second(timedate)), dayofweek(timedate)\"\n",
    "from1 = \" from dataset\"\n",
    "where1 = \" where year(timedate) >= 2017\"\n",
    "groupby = \" group by \"+columns+\", dayofweek(timedate)\"\n",
    "orderby = \" order by \"+columns\n",
    "query = select + from1 +where1+  groupby +orderby\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns1= \"year(timedate), month(timedate), day(timedate), hour(timedate), minute(timedate), second(timedate)\"\n",
    "select1 = \"select \"+columns1+\", count(second(timedate))\"\n",
    "groupby2= \" group by \"+columns1\n",
    "query1 = select1 + from1 + groupby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/13 00:30:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DATA_AS_OF\n",
      " Schema: timedate\n",
      "Expected: timedate but found: DATA_AS_OF\n",
      "CSV file: file:///Users/minoseah629/Downloads/DOT_Traffic_Speeds_NBE.csv\n",
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-------------+--------------+----------------+----------------+-----------------------+\n",
      "|year(timedate)|month(timedate)|day(timedate)|hour(timedate)|minute(timedate)|second(timedate)|count(second(timedate))|\n",
      "+--------------+---------------+-------------+--------------+----------------+----------------+-----------------------+\n",
      "|          2018|             11|           17|            10|              52|              40|                      7|\n",
      "|          2018|             11|           17|            11|              12|              43|                      7|\n",
      "|          2018|             11|           17|            12|              48|              16|                      7|\n",
      "|          2018|             11|           17|            12|              48|              40|                      6|\n",
      "|          2019|              6|            6|             1|              53|               6|                      7|\n",
      "|          2018|             11|           17|            15|               8|              33|                      7|\n",
      "|          2018|             11|           17|            16|              52|              44|                      2|\n",
      "|          2018|             11|           17|            19|              47|              53|                     11|\n",
      "|          2018|             11|           17|            19|              42|              55|                     19|\n",
      "|          2018|             11|           18|             0|              58|              20|                      1|\n",
      "|          2018|             11|           18|             6|              48|              30|                      2|\n",
      "|          2018|             11|           18|             6|              58|              25|                     38|\n",
      "|          2018|             11|           18|            10|              52|              29|                     38|\n",
      "|          2018|             11|           18|            15|              23|              20|                     64|\n",
      "|          2018|             11|           19|             0|              13|              14|                     66|\n",
      "|          2018|             11|           19|             0|              38|              12|                      9|\n",
      "|          2018|             11|           19|             1|               3|              13|                     13|\n",
      "|          2018|             11|           19|             1|              38|              14|                     56|\n",
      "|          2018|             11|           19|             3|               3|              17|                     56|\n",
      "|          2018|             11|           19|             7|              48|              25|                     12|\n",
      "+--------------+---------------+-------------+--------------+----------------+----------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.createOrReplaceTempView(\"dataset\")\n",
    "linkpoints = spark.sql(query1)\n",
    "linkpoints.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/13 00:31:27 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DATA_AS_OF\n",
      " Schema: timedate\n",
      "Expected: timedate but found: DATA_AS_OF\n",
      "CSV file: file:///Users/minoseah629/Downloads/DOT_Traffic_Speeds_NBE.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "linkpoints.write.format('csv').mode('overwrite').save('file:/Users/minoseah629/Desktop/ait614/second.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|max(length(_c6))|\n",
      "+----------------+\n",
      "|             255|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.createOrReplaceTempView(\"noschema\")\n",
    "spark.sql(\"select max(length(_c6)) from noschema\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
