{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install findspark\n",
    "\n",
    "#/usr/local/opt/apache-spark/libexec/sbin\n",
    "#./start-thriftserver.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.server2.thrift.port\n",
      "22/03/30 17:39:12 WARN Utils: Your hostname, Ewins-Mac-Hex.local resolves to a loopback address: 127.0.0.1; using 192.168.0.47 instead (on interface en0)\n",
      "22/03/30 17:39:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/30 17:39:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"spark\").config('spark.driver.memory', '32g').config(\"hive.server2.thrift.port\", 10000).config(\"spark.sql.hive.thriftServer.singleSession\", True).enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Speed: double (nullable = true)\n",
      " |-- TravelTime: integer (nullable = true)\n",
      " |-- timedate: timestamp (nullable = true)\n",
      " |-- LinkId: integer (nullable = true)\n",
      " |-- LinkPoints: string (nullable = true)\n",
      " |-- Owner: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Link_Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType,TimestampType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType()),\n",
    "    StructField(\"Speed\", DoubleType()),\n",
    "    StructField(\"TravelTime\", IntegerType()),\n",
    "    StructField(\"Status\", IntegerType()),\n",
    "    StructField(\"timedate\", StringType()),\n",
    "    StructField(\"LinkId\", IntegerType()),\n",
    "    StructField(\"LinkPoints\", StringType()),\n",
    "    StructField(\"EncodedLinkPoints\", StringType()),\n",
    "    StructField(\"EncodedPolyLineLvls\", StringType()),\n",
    "    StructField(\"Owner\", StringType()),\n",
    "    StructField(\"TranscomId\", IntegerType()),\n",
    "    StructField(\"Borough\", StringType()),\n",
    "    StructField(\"Link_Name\", StringType())\n",
    "])\n",
    "\n",
    "schema1 = \"ID bigint, Speed float, TravelTime int, Status int, TimeDate string, LinkId int, LinkPoints string, EncodedLinkPoints string, EncodedPolyLineLvls string, Owner string, TranscomId int, Borough string,Link_Name string\"\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(\"file:/Users/minoseah629/Downloads/DOT_Traffic_Speeds_NBE.csv\")\n",
    "df2 = df2.withColumn(\"timedate\", to_timestamp(\"timedate\", 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df2 = df2.na.drop()\n",
    "df2 = df2.drop(*(\"Status\",\"EncodedLinkPoints\",\"EncodedPolyLineLvls\",\"TranscomId\"))\n",
    "df2.printSchema()\n",
    "df2.createOrReplaceTempView(\"dataset\")\n",
    "# .option(\"inferSchema\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/30 21:09:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: ID, SPEED, TRAVEL_TIME, STATUS, DATA_AS_OF, LINK_ID, LINK_POINTS, ENCODED_POLY_LINE, ENCODED_POLY_LINE_LVLS, OWNER, TRANSCOM_ID, BOROUGH, LINK_NAME\n",
      " Schema: ID, Speed, TravelTime, Status, timedate, LinkId, LinkPoints, EncodedLinkPoints, EncodedPolyLineLvls, Owner, TranscomId, Borough, Link_Name\n",
      "Expected: TravelTime but found: TRAVEL_TIME\n",
      "CSV file: file:///Users/minoseah629/Downloads/DOT_Traffic_Speeds_NBE.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg(speed)': 34.99334896089561}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg = df2.agg({\"speed\": \"avg\"}).alias('avg').first().asDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.99334896089561"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(avg['avg(speed)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/minoseah629/Desktop/ait614/filter by date copy 2.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/minoseah629/Desktop/ait614/filter%20by%20date%20copy%202.ipynb#ch0000006?line=0'>1</a>\u001b[0m avgString \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(avg[\u001b[39m'\u001b[39m\u001b[39mavg(speed)\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/minoseah629/Desktop/ait614/filter%20by%20date%20copy%202.ipynb#ch0000006?line=1'>2</a>\u001b[0m speed \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcase \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/minoseah629/Desktop/ait614/filter%20by%20date%20copy%202.ipynb#ch0000006?line=2'>3</a>\u001b[0m      \u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhen (\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mavgString\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/2) > speed and speed < \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mavgString\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m  then 0 \u001b[39m\u001b[39m\"\u001b[39m\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/minoseah629/Desktop/ait614/filter%20by%20date%20copy%202.ipynb#ch0000006?line=3'>4</a>\u001b[0m      \u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39melse 1 end \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/minoseah629/Desktop/ait614/filter%20by%20date%20copy%202.ipynb#ch0000006?line=4'>5</a>\u001b[0m      \u001b[39m# +\"when speed > \"+str(avg['avg(speed)'])+\" then 1 \"\\\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avg' is not defined"
     ]
    }
   ],
   "source": [
    "avgString = str(avg['avg(speed)'])\n",
    "speed = \"case \" \\\n",
    "     +\"when (\"+avgString+\"/2) > speed and speed < \"+avgString+\"  then 0 \"\\\n",
    "     +\"else 1 end \"\n",
    "     # +\"when speed > \"+str(avg['avg(speed)'])+\" then 1 \"\\\n",
    "travel = \"(traveltime/60) \"\n",
    "columns = \"id, speed,year(timedate),month(timedate),day(timedate),hour(timedate),minute(timedate), second(timedate), \"+travel\n",
    "fromsql = \" from dataset\"\n",
    "command = \"select \"+columns+ \"as `traveltime in minutes` , \"+speed +\"as `speed group` \"+ fromsql + \" group by \"+ columns\n",
    "display(command)\n",
    "data2022 = spark.sql(command)\n",
    "data2022.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/30 21:43:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: ID, SPEED, TRAVEL_TIME, STATUS, DATA_AS_OF, LINK_ID, LINK_POINTS, ENCODED_POLY_LINE, ENCODED_POLY_LINE_LVLS, OWNER, TRANSCOM_ID, BOROUGH, LINK_NAME\n",
      " Schema: ID, Speed, TravelTime, Status, timedate, LinkId, LinkPoints, EncodedLinkPoints, EncodedPolyLineLvls, Owner, TranscomId, Borough, Link_Name\n",
      "Expected: TravelTime but found: TRAVEL_TIME\n",
      "CSV file: file:///Users/minoseah629/Downloads/DOT_Traffic_Speeds_NBE.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data2022.write.format('csv').mode('overwrite').save('file:/Users/minoseah629/Desktop/ait614/speedgroups.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/22 23:33:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: ID, SPEED, TRAVEL_TIME, STATUS, DATA_AS_OF, LINK_ID, LINK_POINTS, ENCODED_POLY_LINE, ENCODED_POLY_LINE_LVLS, OWNER, TRANSCOM_ID, BOROUGH, LINK_NAME\n",
      " Schema: ID, Speed, TravelTime, Status, timedate, LinkId, LinkPoints, EncodedLinkPoints, EncodedPolyLineLvls, Owner, TranscomId, Borough, Link_Name\n",
      "Expected: TravelTime but found: TRAVEL_TIME\n",
      "CSV file: file:///Users/minoseah629/Downloads/DOT_Traffic_Speeds_NBE.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1893262"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2022.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
