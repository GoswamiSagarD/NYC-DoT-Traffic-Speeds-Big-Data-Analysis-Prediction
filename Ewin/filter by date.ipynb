{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install findspark\n",
    "#/usr/local/opt/apache-spark/libexec/sbin\n",
    "#./start-thriftserver.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.server2.thrift.port\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/22 22:46:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"spark\")\\\n",
    "    .config('spark.driver.memory', '32g')\\\n",
    "    .config(\"hive.server2.thrift.port\", 10000)\\\n",
    "    .config(\"spark.sql.hive.thriftServer.singleSession\", True)\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Speed: double (nullable = true)\n",
      " |-- TravelTime: integer (nullable = true)\n",
      " |-- timedate: timestamp (nullable = true)\n",
      " |-- LinkId: integer (nullable = true)\n",
      " |-- LinkPoints: string (nullable = true)\n",
      " |-- Owner: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Link_Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType,TimestampType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType()),\n",
    "    StructField(\"Speed\", DoubleType()),\n",
    "    StructField(\"TravelTime\", IntegerType()),\n",
    "    StructField(\"Status\", IntegerType()),\n",
    "    StructField(\"timedate\", StringType()),\n",
    "    StructField(\"LinkId\", IntegerType()),\n",
    "    StructField(\"LinkPoints\", StringType()),\n",
    "    StructField(\"EncodedLinkPoints\", StringType()),\n",
    "    StructField(\"EncodedPolyLineLvls\", StringType()),\n",
    "    StructField(\"Owner\", StringType()),\n",
    "    StructField(\"TranscomId\", IntegerType()),\n",
    "    StructField(\"Borough\", StringType()),\n",
    "    StructField(\"Link_Name\", StringType())\n",
    "])\n",
    "\n",
    "schema1 = \"ID bigint, Speed float, TravelTime int, Status int, TimeDate string, LinkId int, LinkPoints string, EncodedLinkPoints string, EncodedPolyLineLvls string, Owner string, TranscomId int, Borough string,Link_Name string\"\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(\"file:/Users/minoseah629/Downloads/DOT_Traffic_Speeds_NBE.csv\")\n",
    "df2 = df2.withColumn(\"timedate\", to_timestamp(\"timedate\", 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df2 = df2.na.drop()\n",
    "df2 = df2.drop(*(\"Status\",\"EncodedLinkPoints\",\"EncodedPolyLineLvls\",\"TranscomId\"))\n",
    "df2.printSchema()\n",
    "# .option(\"inferSchema\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns1= \"year(timedate), month(timedate), day(timedate), hour(timedate), minute(timedate), second(timedate)\"\n",
    "select1 = \"select \"+columns1+\", count(second(timedate))\"\n",
    "from1 = \" from dataset\"\n",
    "groupby2= \" group by \"+columns1\n",
    "query1 = select1 + from1 + groupby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/13 00:30:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DATA_AS_OF\n",
      " Schema: timedate\n",
      "Expected: timedate but found: DATA_AS_OF\n",
      "CSV file: file:///Users/minoseah629/Downloads/DOT_Traffic_Speeds_NBE.csv\n",
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-------------+--------------+----------------+----------------+-----------------------+\n",
      "|year(timedate)|month(timedate)|day(timedate)|hour(timedate)|minute(timedate)|second(timedate)|count(second(timedate))|\n",
      "+--------------+---------------+-------------+--------------+----------------+----------------+-----------------------+\n",
      "|          2018|             11|           17|            10|              52|              40|                      7|\n",
      "|          2018|             11|           17|            11|              12|              43|                      7|\n",
      "|          2018|             11|           17|            12|              48|              16|                      7|\n",
      "|          2018|             11|           17|            12|              48|              40|                      6|\n",
      "|          2019|              6|            6|             1|              53|               6|                      7|\n",
      "|          2018|             11|           17|            15|               8|              33|                      7|\n",
      "|          2018|             11|           17|            16|              52|              44|                      2|\n",
      "|          2018|             11|           17|            19|              47|              53|                     11|\n",
      "|          2018|             11|           17|            19|              42|              55|                     19|\n",
      "|          2018|             11|           18|             0|              58|              20|                      1|\n",
      "|          2018|             11|           18|             6|              48|              30|                      2|\n",
      "|          2018|             11|           18|             6|              58|              25|                     38|\n",
      "|          2018|             11|           18|            10|              52|              29|                     38|\n",
      "|          2018|             11|           18|            15|              23|              20|                     64|\n",
      "|          2018|             11|           19|             0|              13|              14|                     66|\n",
      "|          2018|             11|           19|             0|              38|              12|                      9|\n",
      "|          2018|             11|           19|             1|               3|              13|                     13|\n",
      "|          2018|             11|           19|             1|              38|              14|                     56|\n",
      "|          2018|             11|           19|             3|               3|              17|                     56|\n",
      "|          2018|             11|           19|             7|              48|              25|                     12|\n",
      "+--------------+---------------+-------------+--------------+----------------+----------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.createOrReplaceTempView(\"dataset\")\n",
    "linkpoints = spark.sql(query1)\n",
    "linkpoints.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/13 00:31:27 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DATA_AS_OF\n",
      " Schema: timedate\n",
      "Expected: timedate but found: DATA_AS_OF\n",
      "CSV file: file:///Users/minoseah629/Downloads/DOT_Traffic_Speeds_NBE.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "linkpoints.write.format('csv').mode('overwrite').save('file:/Users/minoseah629/Desktop/ait614/second.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/minoseah629/Desktop/ait614/filter by date.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/minoseah629/Desktop/ait614/filter%20by%20date.ipynb#ch0000008?line=0'>1</a>\u001b[0m spark\u001b[39m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
