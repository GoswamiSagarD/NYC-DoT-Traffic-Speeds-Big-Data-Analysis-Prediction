{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.server2.thrift.port\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/13 13:01:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"spark\")\\\n",
    "    .config('spark.driver.memory', '50g')\\\n",
    "    .config(\"hive.server2.thrift.port\", 10000)\\\n",
    "    .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "    .config(\"spark.sql.hive.thriftServer.singleSession\", True)\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType,TimestampType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType()),\n",
    "    StructField(\"Speed\", DoubleType()),\n",
    "    StructField(\"TravelTime\", DoubleType()),\n",
    "    StructField(\"Status\", IntegerType()),\n",
    "    StructField(\"timedate\", StringType()),\n",
    "    StructField(\"LinkId\", IntegerType()),\n",
    "    StructField(\"LinkPoints\", StringType()),\n",
    "    StructField(\"EncodedLinkPoints\", StringType()),\n",
    "    StructField(\"EncodedPolyLineLvls\", StringType()),\n",
    "    StructField(\"Owner\", StringType()),\n",
    "    StructField(\"TranscomId\", IntegerType()),\n",
    "    StructField(\"Borough\", StringType()),\n",
    "    StructField(\"Link_Name\", StringType())    \n",
    "])\n",
    "\n",
    "schema1 = \"ID bigint, Speed float, TravelTime int, Status int, TimeDate string, LinkId int, LinkPoints string, EncodedLinkPoints string, EncodedPolyLineLvls string, Owner string, TranscomId int, Borough string,Link_Name string\"\n",
    "training = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .schema(schema) \\\n",
    "    .load(\"file:/Users/minoseah629/Downloads/DOT_Traffic_Speeds_NBE.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Speed: double (nullable = true)\n",
      " |-- TravelTime: double (nullable = true)\n",
      " |-- timedate: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Link_Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols= (\"ID\",\"Status\",\"LinkId\",\"LinkPoints\",\"EncodedLinkPoints\",\"EncodedPolyLineLvls\",\"TranscomId\", \"Owner\")\n",
    "training = training.drop(*cols)\n",
    "training.printSchema()\n",
    "#training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/13 13:01:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SPEED, TRAVEL_TIME, DATA_AS_OF, BOROUGH, LINK_NAME\n",
      " Schema: Speed, TravelTime, timedate, Borough, Link_Name\n",
      "Expected: TravelTime but found: TRAVEL_TIME\n",
      "CSV file: file:///Users/minoseah629/Downloads/DOT_Traffic_Speeds_NBE.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/minoseah629/Desktop/ait614/logisticregression.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/minoseah629/Desktop/ait614/logisticregression.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/minoseah629/Desktop/ait614/logisticregression.ipynb#ch0000004?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstat\u001b[39;00m \u001b[39mimport\u001b[39;00m Statistics\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/minoseah629/Desktop/ait614/logisticregression.ipynb#ch0000004?line=3'>4</a>\u001b[0m summary \u001b[39m=\u001b[39m Statistics\u001b[39m.\u001b[39;49mcolStats(training\u001b[39m.\u001b[39;49mtoPandas())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/minoseah629/Desktop/ait614/logisticregression.ipynb#ch0000004?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(summary\u001b[39m.\u001b[39mmean())  \u001b[39m# a dense vector containing the mean value for each column\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/minoseah629/Desktop/ait614/logisticregression.ipynb#ch0000004?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(summary\u001b[39m.\u001b[39mvariance())  \u001b[39m# column-wise variance\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/mllib/stat/_statistics.py:99\u001b[0m, in \u001b[0;36mStatistics.colStats\u001b[0;34m(rdd)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/opt/apache-spark/libexec/python/pyspark/mllib/stat/_statistics.py?line=62'>63</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     <a href='file:///usr/local/opt/apache-spark/libexec/python/pyspark/mllib/stat/_statistics.py?line=63'>64</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcolStats\u001b[39m(rdd):\n\u001b[1;32m     <a href='file:///usr/local/opt/apache-spark/libexec/python/pyspark/mllib/stat/_statistics.py?line=64'>65</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/opt/apache-spark/libexec/python/pyspark/mllib/stat/_statistics.py?line=65'>66</a>\u001b[0m \u001b[39m    Computes column-wise summary statistics for the input RDD[Vector].\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/opt/apache-spark/libexec/python/pyspark/mllib/stat/_statistics.py?line=66'>67</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/opt/apache-spark/libexec/python/pyspark/mllib/stat/_statistics.py?line=96'>97</a>\u001b[0m \u001b[39m    array([ 2.,  0.,  0., -2.])\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/opt/apache-spark/libexec/python/pyspark/mllib/stat/_statistics.py?line=97'>98</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///usr/local/opt/apache-spark/libexec/python/pyspark/mllib/stat/_statistics.py?line=98'>99</a>\u001b[0m     cStats \u001b[39m=\u001b[39m callMLlibFunc(\u001b[39m\"\u001b[39m\u001b[39mcolStats\u001b[39m\u001b[39m\"\u001b[39m, rdd\u001b[39m.\u001b[39;49mmap(_convert_to_vector))\n\u001b[1;32m    <a href='file:///usr/local/opt/apache-spark/libexec/python/pyspark/mllib/stat/_statistics.py?line=99'>100</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m MultivariateStatisticalSummary(cStats)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py:5583\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py?line=5575'>5576</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py?line=5576'>5577</a>\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py?line=5577'>5578</a>\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py?line=5578'>5579</a>\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py?line=5579'>5580</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py?line=5580'>5581</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py?line=5581'>5582</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py?line=5582'>5583</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "summary = Statistics.colStats(training.toPandas())\n",
    "print(summary.mean())  # a dense vector containing the mean value for each column\n",
    "print(summary.variance())  # column-wise variance\n",
    "print(summary.numNonzeros())  # number of nonzeros in each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
